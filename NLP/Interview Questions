1. What is part of speech (POS) tagging? What is the simplest approach to building a POS tagger that you can imagine
Given a sequenece of words, generate the POS tag sequence. Current POS of the word, depends only on current word or previous tag.

2. How would you build a POS tagger from scratch given a corpus of annotated sentences? How would you deal with unknown words?
HMM. Smoothing. The basic idea here is that for unknown words more probability mass should be given to tags that appear with a wider 
variety of low frequency words. For example, since the tag NOUN appears on a large number of different words and DETERMINER appears
on a small number of different words, it is more likely that an unseen word will be a NOUN.

3. How would you train a model that identifies whether the word “Apple” in a sentence belongs to the fruit or the company
Split the sentence into words, normalise them, build a dictionary
With each word, store how many times they occurred in tweets about the company, and how many times they appeared in tweets about the fruit 
- these tweets must be confirmed by a human. When a new tweet comes in, find every word in the tweet in the dictionary, calculate a 
weighted score - words that are used frequently in relation to the company would get a high company score, and vice versa; words used 
rarely, or used with both the company and the fruit, would not have much of a score.
In other words, use Bayesian Classification( like one used for Email filtering).

4. How would you find all the occurrences of quoted text in a news article?
Regular expression matching

5. How would you build a system that auto corrects text that has been generated by a speech recognition system?
- use language model to find the probabilities and order the results based on the probabilities. 

6. What is latent semantic indexing and where can it be applied?
Mathematical method used to determine the relationship between terms and concepts in content.
Examines the document collection as a whole, to see which other documents contain some of those same words.
Uses : To give similar articles although they might have different words
Eg: Search one player of a club, another player of the same club is suggested

7. How would you build a system to translate English text to Greek and vice-versa?

8. How would you design a model to predict whether a movie review was positive or negative?
Sentiment analysis. 
If you don't want to get in to a very complicated problem, you can just create lists of "positive" and "negative" words (and weight them if you want) 
and do word counts on sections of text. Obviously this isn't a "smart" solution, but it gets you some information with very little work, where doing serious NLP would be very time consuming.
Mainly the accuracy depends upon pre-processing steps, features extracted and the learning model used.

Pre-processing steps normally includes removal of stop words and that is fine. 
Features extraction is of various methods. Word embeddings is gaining its popularity in NLP, due to its interesting characteristics of vectors generated. 
There are lot of learning models from naive bayes, svm to neural network models. The accuracy of it depends upon the dataset used and the features generated and so each models need to be tested under trial and error method. sklearn provides a nice support for ML models

9. How would you build a system that automatically groups news articles by subject?

First, you are going to want to figure out what properties of the news stories you want to group based on. A common technique is to use 'word bags': 
just a list of the words that appear in the body of the story or in the title. You can do some additional processing such as removing common English 
"stop words" that provide no meaning, such as "the", "because". You can even do porter stemming to remove redundancies with plural words and word endings 
such as "-ion". This list of words is the feature vector of each document and will be used to measure similarity. You may have to do some preprocessing to remove html markup.

Second, you have to define a similarity metric: similar stories score high in similarity. Going along with the bag of words approach, two stories are similar 
if they have similar words in them (I'm being vague here, because there are tons of things you can try, and you'll have to see which works best).

Finally, you can use a classic clustering algorithm, such as k-means clustering, which groups the stories together, based on the similarity metric.
In summary: convert news story into a feature vector -> define a similarity metric based on this feature vector -> unsupervised clustering.

10. What is entropy? How would you estimate the entropy of the English language?
Entropy is a measure of the uncertainty associated with a random variable.
Higher entropy = more uncertain / harder to predict

In case of NLP, each word corresponds to a random variable.
Obtain a representative sample of English text (perhaps a carefully selected corpus of news articles, blog posts, some scientific articles and some personal letters), as large as possible
Iterate through its characters and count the frequency of occurrence of each of them
Use the frequency, divided by the total number of characters, as estimate for each character's probability
Calculate the average length in bits of each character by multiplying its probability with the negative logarithm of that same probability (the base-2 logarithm if we want the unit of entropy to be bit)
Take the sum of all average lengths of all characters. That is the result.

11. What is the TF-IDF score of a word and in what context is this useful?
 tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.
 The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to 
 adjust for the fact that some words appear more frequently in general.
 The weight of a term that occurs in a document is simply proportional to the term frequency
 Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set 
 and increases the weight of terms that occur rarely.
